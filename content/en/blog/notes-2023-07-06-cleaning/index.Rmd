---
title: "Project notes: Cleaning data"
output: blogdown::html_page
description: "Lecture notes for cleaning project data."
excerpt: "Lecture notes for cleaning project data."
date: 2023-07-06
lastmod: "`r Sys.Date()`"
draft: false
images: []
categories: ["Class notes"]
tags: []
contributors: ["Monica Thieu"]
pinned: false
homepage: false
---

Again, now that we're fully in the project work stage of the course, the lecture notes after class will focus mainly on demonstrating and explaining specific techniques that people need for their particular project data.

```{r}
library(tidyverse)
```

First, I'll read in some student data that will pop up in later examples.

```{r}
csv_data <- read_csv(here::here("ignore",
                                "data",
                                "AI_index_db.csv"))
```

## Renaming columns

Often times, when you read in other people's data, they may have nice-looking column names that _are not legal variable names,_ usually because the column names have spaces in them.

```{r}
csv_data
```

See how some of the column names have spaces in them?

You don't _HAVE_ to rename these columns, because you can still call them using the backtick character ` as special quotation marks around the column name. Quoting out a column name with backticks in R code tells R that that column name does indeed represent ONE column name/variable, even if normally a space would mean you're ending one command name and starting another.

This can be annoying, though, so if you want to rename column names to remove spaces, here's a smooth way to do it:

```{r}
csv_data <- csv_data |> 
  rename_with(.fn = \(x) str_replace_all(x, " ", "_"),
              .cols = everything())
```

You may recognize some of these code bits from other techniques like using `across()` inside `mutate()` to manipulate multiple columns at once. `rename_with()` is like `across()` but for renaming many columns at once, using a string function on the column names.

The `.fn` argument takes the function you want to pass over each column name. In this example, I'm using `str_replace_all()` to replace every space character in a column name with an underscore, to make it a legal variable name. You could even remove spaces by using `str_remove_all()` instead, but sometimes this makes column names harder to read. We always feed this function into the `.fn` argument starting with `\(x)`, which is necessary to tell R "pass this string function across every column name, one at a time".

Then, the `.cols` argument tells R which column names we want to rename. In this case, our function should only affect the names of columns that have spaces in them (otherwise, nothing gets replaced), so we can call the function on every single column using `everything()`. That way, the columns that need to get fixed get fixed, and the columns that don't need to get fixed stay the same.

Now, all column names are legal variable names, and we don't have to use the backtick quotes anymore.

```{r}
csv_data
```


## Dropping columns with `select()`

Refer to the [previous class notes on select()](../class-notes-june-15-2023/#select-choose-a-subset-of-columns) to see a variety of ways you can use `select()` to choose just some columns of your data.

Another way you can use `select()` that I didn't show in the earlier class is to _drop just some unwanted columns, keeping all the rest._

You can use the minus `-` in front of the column name to designate columns to _drop._ All columns not mentioned will remain in the data.

For example:

```{r}
csv_data <- csv_data |> 
  select(-Cluster, -Political_regime)
```

Those columns should now be gone:

```{r}
csv_data
```

## Dropping rows with `filter()`

Refer to the [previous class notes on `filter()`](../class-notes-june-15-2023/#filter-choose-a-subset-of-rows) to see different logical & relational statements you can use to choose only the rows in your data you want to keep.

In particular, you might want to use `%in%` to keep only the rows of your data where the values in a particular column are in a list of interest. For example:

```{r}
csv_data
```

Right now, we see this data has row for `r nrow(csv_data)` countries. Maybe you don't care about all those countries. In that case, you can use %in%, with a vector of countries you want to keep, specified with `c("Country 1", "Country 2", etc)`. Remember to spell the country names (or whatever other level names you want to keep) exactly how they're spelled in the data! 

```{r}
csv_data_filtered <- csv_data |> 
  filter(Country %in% c("United States of America", "Canada", "Mexico"))

csv_data_filtered
```

## Fixing number columns encoded as character for some reason

This is one of the more common reasons you might need to clean data. R is normally pretty good at recognizing columns of numbers and storing them as numeric data. However, here are a few instances in which you might need to repair data when a number column is being stored as character.

### The missing value placeholder is being read in as text

R is totally fine with having `NA` values in otherwise numeric columns, but if R reads those missing values in from your dataset file as _text,_ the entire column will be read in as text to be safe. Check out the [project notes on reading in data](../project-notes-reading-in-data/#other-arguments-you-may-need) to see how to use the `na` argument in your file-reading function to render those placeholder values as `NA` and allow the rest of the column to read in as numeric.

### The column names are getting read in as observations

Again, if R detects even _one_ text-like value in a column of a dataset file, it will read the entire column in as text data to be safe. This means that sometimes columns will read in as text when _the column name is not read in as the column name, but instead as the first row of data._ This might happen if you have extra rows of non-data text on the top of your CSV or other plain-text data file. Refer to the [project notes on reading in data](../project-notes-reading-in-data/#other-arguments-you-may-need) to see how to use the `skip` argument in your file-reading function to skip those rows and read in the column names as the top row of the file, no matter where it might actually be.

### The numbers are spelled in an unexpected way

By default, R expects numbers in data files to be formatted according to American English defaults. So, for example, if you read in a column of numbers where each number value contains _only_ digits, R unambiguously knows how to read these in.

```{r}
# Don't worry about the syntax in these first two lines
# I am using these as an example to show you
# how read_csv() will interpret numbers spelled this way in your data file
c("value", "1000", "2000", "3000") |> 
  I() |> 
  # I am piping the data into read_csv()
  # so I don't have to manually specify the first argument
  read_csv()
```

Specific to American English, R also knows that the comma is often used to break up every third place value of digits, so data containing numbers with commas will still get read in as numeric data.

```{r}
c("value", "1,000", "2,000", "3,000") |> 
  I() |> 
  read_csv()
```
R also knows that the decimal is specified with a point in American English, so numbers with periods get read in as decimal values.

```{r}
c("value", "1.01", "2.01", "3.01") |> 
  I() |> 
  read_csv()
```
However, sometimes you might need to read in data with different thousands-grouping or decimal markers. For example, in Europe, they have the decimal and the grouping markers switched. One thousand is spelled like 1.000, and one-point-five is spelled like 1,5 (if you have seen a gas station outside the US, this might be familiar to you). Or, for example, you might have data where the grouping marker for the thousands is a space instead of a comma OR a period. The space is considered an ["international" thousands-grouping marker,](https://www.bipm.org/en/committees/cg/cgpm/22-2003/resolution-10) because it can't be confused for the other two.

These number-spelling differences are considered part of the **locale** of your data, because they are regional. R's default locale is the US, so we normally don't notice anything odd when we use R, but you can always manually specify a locale setting when you read in data spelled according to the conventions of a different locale. Use the `locale` argument of `read_csv()` (or `read_excel()`, or any functions in the family) to manually specify what the decimal mark is and what the thousands-grouping mark is.

For example, these should get read in as the numbers 1, 2, and 3 if we tell R that the decimal is a comma and the thousands-grouping is a period. (If you need to tell R _either_ that the decimal is a comma _or_ that the thousands-grouping is a period, I recommend setting both at the same time so R doesn't get confused with the American English defaults.)

```{r}
c("value", "1,000", "2,000", "3,000") |> 
  I() |> 
  read_csv(locale = locale(decimal_mark = ",",
                           grouping_mark = "."))
```

And these should get read in as regular thousands if we tell R that the thousands-grouping mark is a space, per international weights and measures standards. (We only need to specify the thousands-grouping mark because it should not affect the existing default decimal mark this time.)

```{r}
c("value", "1 000", "2 000", "3 000") |> 
  I() |> 
  read_csv(locale = locale(grouping_mark = " "))
```

You can use the `locale` argument along with all the other typical arguments you might set in your file-reading function.

## Recoding categorical variables with `mutate()` and `fct_recode()`

Check out [the previous class notes](../class-notes-june-12-2023/#recoding-categorical-columns) for the detailed description of using `fct_recode()` inside `mutate()` to recode categorical variables.

A few reminders:

The levels always go `"new_level" = "old_level"`. In general, programming syntax order reads right to left, so it's old on the right turning into new on the left!

You only need to do `fct_recode(as.character(YOUR_CATEGORICAL_COLUMN))` if you're starting with a _numeric_ column where _the numbers represent categorical levels._ If your starting column is ALREADY categorical text, you do not need to coerce it to character again.

## Reshaping data with `pivot_longer()`

(To learn more about the technique presented in this section, refer to [section 6.3 of the textbook.](https://r4ds.hadley.nz/data-tidy.html#sec-pivoting))

If you are dealing with data collected at multiple points in time, you may sometimes read in data where each data column contains data from the _same variable,_ but measured at _different points in time._

For example, in these data we see that the maternal mortality rate has been measured in a number of countries every 5 years from 2000 to 2020. Each year's column contains data from the _same variable._

```{r}
# Need readxl to read in excel files
library(readxl)

excel_data <- read_excel(here::here("ignore",
                                    "data",
                                    "MMR-maternal-deaths-and-LTR_MMEIG-trends_2000-2020_released-Feb_2023.xlsx"),
                         sheet = 2,
                         range = "A5:H190")
```

```{r}
excel_data
```

To work with these data in R, we want to _reshape_ them so that there is _only one column containing the maternal mortality rate data._ Right now, each row contains all the observations for a particular country, but we need to make the data _longer in rows and narrower in columns_ (or, more hot-dog shaped if you like) so that each row contains the observations for a particular country _in a particular year._ This will let us analyze data _either_ by country _or_ by year (or both).

To reshape the data and make it longer, we will use the function `pivot_longer()`.

Once you pipe your data into `pivot_longer()`, you _always_ need to set the following three arguments:

1. `cols`: Which columns contain the repeated-measure data that you need to hot-dog-ify? Here, because all the year columns are adjacent to one another, we can use the colon `:` to go _from_ the 2000 column `through` the 2020 column. (You can select columns using any valid syntax you might use to choose columns inside `select()`.) Notice that the year column names are quoted with backtick quotes, otherwise R would treat them as numbers and not column names.
1. `names_to`: What do you want the new name of the column containing the old column names to be? In quotes.
1. `values_to`: What do you want the new name of the column containing the actual observations to be? In quotes.

Here, I've also set the `names_transform` argument so that the new `Year` column will get turned into numbers, because that column would be text by default. R expects column names to be text, so the `names_to` column comes out as text by default unless you tell R otherwise.

```{r}
excel_data_long <- excel_data |> 
  pivot_longer(cols = `2000`:`2020`,
               names_to = "Year",
               values_to = "MMR",
               names_transform = list(Year = as.numeric))
```

And now the data have one long column for maternal mortality rate, with identifying columns _both_ for country _and_ for year the data were collected!

```{r}
excel_data_long
```
