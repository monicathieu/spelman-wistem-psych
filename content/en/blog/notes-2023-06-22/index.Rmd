---
title: 'Class notes: June 22, 2023'
output: blogdown::html_page
description: "Lecture notes and helpful info after class."
excerpt: "Lecture notes and helpful info after class."
date: 2023-06-22
lastmod: "`r Sys.Date()`"
draft: false
images: []
categories: ["Class notes"]
tags: []
contributors: ["Monica Thieu"]
pinned: false
homepage: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

## Lecture notes

Through the rest of the course, we will start touching on topics across the R for Data Science textbook. A lot of today's topics are still mentioned in chapter 4, but you will also benefit from reading more about specific techniques from chapters 13-20.

### Read in the data

First, we need to read in the _newer, larger_ Big 5 personality dataset from [openpsychometrics.org](https://openpsychometrics.org/_rawdata/).

This dataset file also has its values separated with tabs, so we can use `read_tsv()` like we did before (that **t** stands for tab).

Note that the path I am using to read in this data is the path on my local computer, and it won't work for you if you directly copy and paste! You will need to change this to the path to where the data are saved on _your_ RStudio Cloud project, relative to _your_ RStudio Cloud home folder.

```{r}
big5_2018_raw <- read_tsv(here::here("ignore", "data", "IPIP-FFM-data-8Nov2018", "data-final.csv"))
```
The next sections will cover our **data cleaning to-do list**, and one example solution to each point on the to-do list.

### Why are all the columns reading in as character data?

When we first attempt to read in the data using all the default settings of `read_tsv()`, almost all of the columns are getting read in as character (or text) data. That's weird! The personality columns should all be numbers...

```{r}
big5_2018_raw
```

We can use `count()` to check what levels are in one of the personality question columns that's supposed to be numeric, to check for any sneaky text values in there that are causing the whole column to read in as text.

```{r}
big5_2018_raw |> 
  count(EXT1)
```

Aha! This "NULL" value must be the problem. If we use `filter()` to get only the data that has "NULL" in the first column, we can see it clearly represents missing data across all the columns.

```{r}
big5_2018_raw |> 
  filter(EXT1 == "NULL")
```

Let's fix this by adding an argument to `read_tsv()`. The `na` argument allows us to specify any values that we _know_ represent missing data, so that R will read them in as `NA` and not as text.

```{r}
big5_2018_raw <- read_tsv(here::here("ignore", "data", "IPIP-FFM-data-8Nov2018", "data-final.csv"),
                          na = "NULL")
```

Let's also use `filter()` to remove the rows of every participant who has `NA` in the personality columns, because they actually have no real data.

The logical statement `!is.na(EXT1)` allows us to keep only the rows where the EXT1 column is _not_ missing (the not comes from the !).

```{r}
big5_2018 <- big5_2018_raw |> 
  filter(!is.na(EXT1))
```

### Can we drop some columns to save memory?

By reading the codebook, we can see that the only data columns are the personality question columns, and the reaction time columns for each question. I don't have plans to analyze the reaction time data, so we can use `select()` to keep only the personality question columns.

I can select from `EXT1:OPN10` because I know all those columns are adjacent to one another.

```{r}
big5_2018 <- big5_2018 |> 
  select(EXT1:OPN10)
```

### What do the 0 personality responses represent?

The codebook file tells us that responses on each personality question go from 1 to 5, just like in the previous dataset. But we saw earlier that some values are 0... what do we think this means?

If we look at some rows where participants have a response of 0 for `EXT1`, we can see that it looks like 0 _also_ represents **missing** data. However, some of these people do have responses for some questions and not others, not like "NULL" when every single response was missing.

```{r}
big5_2018 |> 
  filter(EXT1 == 0)
```

### Counting up how many actual responses each person has

We can apply some of the techniques we used before, with some new functions as well, to turn each of the 50 personality question columns into 0/1 _binary_ data, with 1 whenever the participant DID say something (anything from 1-5). Then, we can count how many 1 values each participant has across all 50 columns, to count up the number of questions people actually responded to.

We will do this by creating a _new_ dataframe JUST to hold the question count column.

```{r}
big5_missing_counts <- big5_2018 |> 
  # First, use across() in mutate() to turn each personality col to binary
  # Use 1 if they DID respond, and 0 if they did NOT
  mutate(
    across(.cols = everything(),
           .fns = \(x) if_else(x > 0, 1, 0))
  ) |> 
  # Second, use special mutate() to COUNT the 1 values across the binary cols
  rowwise() |> 
  mutate(n_responded = sum(c_across(everything()))) |> 
  ungroup() |> 
  # Finally, drop everything except the count of responded questions
  select(n_responded)
```

```{r}
big5_missing_counts
```

This concludes what we got to at the end of class. Everything below is how I would actually finish the data cleaning, but we didn't get to it as a group. I'm including it here so you can follow my train of thought all the way to the final clean data.

## The rest of the data cleaning (that we didn't finish in class)

### Binding the response count column back onto the main data

Now that we have the response counts in a separate dataframe, we need to stick it horizontally onto the end of our main dataframe, so that we can use it to actually filter our main data later.

We can stick these dataframes together using `bind_cols()`, which will combine two dataframes _horizontally._ **They need to have the same number of rows for this to work!** We know here that they should have the same number of rows because we made `big5_missing_counts` from `big5_2018_raw` above, without dropping any rows.

`bind_cols()` takes the input dataframes as separate arguments, divided with commas.

After that, I'm piping into `select()` so I can move our new column all the way to the left. That way, it'll pop up when we print out the dataframe so I can see what's in that column.

```{r}
# I'm overwriting the previous big5_2018 with the bind_cols version
big5_2018 <- bind_cols(big5_2018, big5_missing_counts) |> 
  select(n_responded, everything())
```

```{r}
big5_2018
```

### Filtering out people with too many missing responses

Now, we can use `filter()` to filter out participants whose numbers are too low. Let's see what happens when we keep only people who responded to all 50 questions.

```{r}
big5_2018 <- big5_2018 |> 
  filter(n_responded == 50)
```

```{r}
big5_2018
```

That gives us `r nrow(big5_2018)` rows remaining, when we started with `r nrow(big5_2018_raw)` rows. I think we'll be okay with the remaining data.

### Calculating sum scores for each of the 5 personality factors

This we already know how to do, from the previous dataset!

```{r}
big5_2018 <- big5_2018 |> 
  rowwise() |> 
  mutate(CSN_sum = sum(c_across(CSN1:CSN10)),
         AGR_sum = sum(c_across(AGR1:AGR10)),
         EST_sum = sum(c_across(EST1:EST10)),
         OPN_sum = sum(c_across(OPN1:OPN10)),
         EXT_sum = sum(c_across(EXT1:EXT10))) |> 
  ungroup() |> 
  select(ends_with("sum"))
```

```{r}
big5_2018
```

Now this dataframe is actually ready for analysis. ðŸŽ‰

